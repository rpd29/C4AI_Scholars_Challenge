{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRUrAHlZKdWd"
      },
      "source": [
        "# C4AI Scholars Program Takehome Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY4wvFv6lTbY"
      },
      "source": [
        "### Background\n",
        "\n",
        "Welcome to the C4AI Scholars Program Take-Home Challenge! This exercise is designed to allow you to showcase your engineering and problem solving skills. The Challenge consists of three parts:\n",
        "\n",
        "* Part One of the challenge requires identifying bugs, and getting the code working. This is designed to test your ability to grapple with real world engineering challenges.\n",
        "* Part Two of the challenge tests your ability to generate code for a specified problem. \n",
        "* Part Three of the challenge is an opportunity for you to attempt an optional challenge question that extends the original problem set. \n",
        "\n",
        "\n",
        "These tasks were chosen as a setting to see how you think about problems, even if they are not in your own research field of interest. The tasks and dataset are not meant to be indicative of the research goals of the Scholar Program. We purposefully have selected a simple toy problem so the focus is on how you think, and does not require significant machine learning resources. \n",
        "\n",
        "Good luck! If you have questions about the framing of the questions, please contact info@for.ai  \n",
        "\n",
        "\n",
        "### How to Use and Submit this Document\n",
        "\n",
        "* **Make a copy of this document** and **rename** it **Firstname_Lastname_C4AIScholarsChallenge**\n",
        "* Once you‚Äôve completed all tasks, **save and pin your revisions**\n",
        "* **Submit a link** to your final document via the [Cohere For AI Scholars Program application](https://jobs.lever.co/cohere/?department=Cohere%20For%20AI). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmbPmOHhLx4U"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc-enDOhazoe"
      },
      "source": [
        "## Overview of Singular Value Decomposition\n",
        "\n",
        "In this takehome, you will be working on a problem involving singular value decomposition. Singular Value Decomposition (SVD) exists for every rectangular matrix. The nice thing about SVD is that the original matrix can be expressed as the sum of outer products of left and singular vectors scaled by the corresponding singular values. Formally:\n",
        "\n",
        "> Let ùõ¢ be a rectangular matrix of dimensions ùëöùòπùëõ, then the SVD of the matrix A is given by $ A = Uùõ¥V^T$ where $U$ is an orthogonal matrix of shape mxm containing the left singular vectors, $V$ is an orthogonal matrix of shape nxn containing the right singular vectors and $ùõ¥$ is a diagonal matrix containing the singular values of $A$. This formulation of SVD can be re-expressed as \\begin{align} A = \\sum_{i=1}^{r} s_i. u_i v_i^T \\end{align} where $r = \\text{min}(m,n)$ represents the rank of the matrix, $s_i$ is the $i$th singular value and $u_i v_i^T$ is the outer product of the $i$th left and right singular vectors. \n",
        "\n",
        "<!-- \\begin{align}\n",
        "A = \\sum_{i=1}^{\\text{min}(m,n)} s_i. u_i v_i^T\n",
        "\\end{align}\n",
        "\\begin{align} -->\n",
        "\n",
        "> The singular values $ùõ¥$ are decreasing in order. So, each outer product is scaled by a smaller value as we compute each term in the sum above. This gives us an opportunity to approximate $A$ using only the sum of the first $k$ outer products where $k < \\text{min}(m,n)$ $-$ this effectively means that we are zero-ing out some of the singular values by assuming that the contribution to the sum is negligible. This is called low-rank approximation.\n",
        "\n",
        "If you aren't familiar with singular value decomposition, or the above feels rusty, don't worry. Take a moment to brush up your knowledge using any of the following resources:\n",
        "* [stanford lecture notes on low rank approximations](https://web.stanford.edu/class/cs168/l/l9.pdf)\n",
        "* [youtube series of short and beginner friendly lectures](https://www.youtube.com/watch?v=gXbThCXjZFM&list=PLMrJAkhIeNNSVjnsviglFoY2nXildDCcv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcL7u9wJjPF3"
      },
      "source": [
        "## Check for understanding (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v-iHTH3nasL"
      },
      "source": [
        "#### Q1: What are some real world applications of low rank approximations?\n",
        "\n",
        "\n",
        "#### Answer: \n",
        "\n",
        "One real-world application of low rank approximations is (lossy) data compression. In particular, if a dataset can be represented as a matrix, then low rank approximation can be used to find another matrix that is ‚Äúclose‚Äù to the original matrix (where the ‚Äúdistance‚Äù between the matrices is measured by the Frobenius norm) but that can be represented more compactly/using less space than the original matrix. For example, an image can typically be represented as a matrix (or a collection of matrices). Low rank approximation can be used to find an image that is ‚Äúclose‚Äù to the original image (perhaps even indistinguishable to the human eye) but that can be represented much more compactly, and therefore occupy less space in memory or persistent storage.\n",
        "\n",
        "Another real-world application of low rank approximation is de-noising. Consider again an image whose data can be represented as a matrix of numbers. The image data may be somewhat ‚Äúnoisy‚Äù due to, for example, electronic noise in the circuits of the camera that captured it. One potential way to ‚Äúdenoise‚Äù the image is to: (1) compute the singular value decomposition of the image matrix, (2) identify the largest/most important singular values, and (3) compute a low-rank approximation to the original matrix using only these largest singular values and the corresponding left and right singular vectors. The assumption of this approach to denoising is that the smallest singular values and their corresponding rank-1 matrices represent the ‚Äúnoise‚Äù in an image and should therefore be eliminated (this assumption may not be true, however, if there is a lot of noise relative to ‚Äúsignal‚Äù in the image).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHGqpn3tkFRL"
      },
      "source": [
        "#### Q2: What are the benefits of compressing a deep neural network? How would you measure the benefits of compression?\n",
        "\n",
        "\n",
        "#### Answer: \n",
        "\n",
        "When using a neural network in an application (e.g. as part of a robot‚Äôs perceptual system, in a content selection algorithm, in a text generation algorithm, etc.), there are several things that relevant stakeholders (e.g. users, a company‚Äôs shareholders, society at large etc.) might care about, including: \n",
        "\n",
        "1) Inference latency (i.e. how long it takes to execute a single inference). For example, an autonomous car needs to be able to quickly react to changes in its environment, so it's important that it is able to quickly process raw perceptual data and make decisions.\n",
        "\n",
        "2) Inference throughput (i.e. how many inferences a system can execute per unit of time). For example, a website that enables people to generate images from text may want to be able to service the requests of large numbers of users simultaneously, in which case the people developing the website might care about inference throughput (including increasing inference throughput without needing to add significant additional computational resources). \n",
        "\n",
        "3) Computational costs and environmental impact. People and organizations typically prefer to minimize the amount of money/resources they need to spend to achieve a given goal/result. Additionally, people and organizations might care about minimizing/reducing any negative impact their activities have on the environment. As a result, people and organizations using neural networks may care about reducing the computational resources and energy required to do inference, which would in turn reduce the costs and environmental impact associated with using them. Using more efficient algorithms that require fewer and/or less expensive/energy intensive operations can reduce both costs and environmental footprint. In addition, costs can also be reduced by reducing the amount of persistent storage required to store the code and data associated with a program/application (including, for example, information about the weights and architecture of a neural network the program uses). \n",
        "\n",
        "4) Quality of the inferences/predictions. Typically the point of using a neural network (or any other sort of model) is to make useful predictions about some aspect of the world (e.g. which content a user of a social media platform is most likely to engage with, what are the most likely future trajectories of a pedestrian crossing the road, which moves in a game of chess are the most likely to lead to a win, etc.). Given this, the quality/accuracy of the predictions a neural network makes greatly affects its utility to users.\n",
        "\n",
        "\n",
        "Compressing deep neural networks can potentially help improve all of the above things. First, compressing a neural network can potentially enable lower latency inference. In particular, inference using a compressed representation of a neural network may require fetching less data from memory, which may significantly speed up inference given how slow memory references can be. Additionally, if the same neural network is used multiple times in quick succession, later inferences might be faster than the first inference if more of the data needed to execute subsequent inferences (e.g. the network weights) is located higher in the memory hierarchy than it was when the first inference was executed (e.g. in L1, L2, or L3 cache versus in main memory). Using a compressed representation of a neural network could help to facilitate this, since a larger fraction of the data associated with the neural network may fit in higher levels of the memory hierarchy, and a smaller fraction of it would need to be replaced/swapped out between inferences. Additionally, in some cases neural network compression can reduce the number of arithmetic operations required to conduct inference, which can further reduce inference latency. Reducing inference latency would also increase inference throughput (though the reverse is not always true). \n",
        "\n",
        "Compressing a deep neural network can also potentially reduce the costs and environmental impact associated with using it. From what I understand, memory references tend to be significantly more energy intensive than other types of operations. By reducing the memory references required to use a neural network, compression could potentially significantly reduce the energy neural network inference consumes. In cases where neural network compression reduces the number of arithmetic operations required to do inference, this would further reduce the computational and energy resources required to use the neural network. In addition, compressing a neural network by definition reduces the amount of space it occupies in persistent storage, which can further reduce costs associated with using it.\n",
        "\n",
        "Last, compressing a deep neural network may potentially help select better models that will make higher quality/more accurate predictions. In particular, a common concern in machine learning and statistics is that in some cases a model fits observations/data well not because it identifies general patterns in the structure of the data, but rather because it in some sense ‚Äúmemorizes‚Äù the data (this phenomenon is sometimes called ‚Äúoverfitting‚Äù). For example, a model might have a separate narrowly applicable rule accounting for each individual observation rather than a single more general rule that seems to account well for all observations. The worry is that a model that overfits observations may not generalize well beyond the situations in the training data, since the components of an overfit model that cause it to fit observations well are narrow patterns that are highly specific/local to one or a small number of observations, rather than more general patterns that match the global structure of the data. In other words, an overfit hypothesis doesn‚Äôt even identify patterns that generalize within the situations in the training data, so you might reasonably expect that such a hypothesis is unlikely to generalize to novel situations. \n",
        "\n",
        "One way to reduce the risk of overfitting is to penalize models with higher ‚Äúdescriptive complexity‚Äù in the learning/training/model selection process (roughly speaking, the ‚Äúdescriptive complexity‚Äù of a hypothesis is the minimum number of symbols required to precisely describe it in some formal language (e.g. a computer programming language like Python)). The theory/intuition underlying this approach is that models with low descriptive complexity (i.e. models that can be described in a small number of symbols) are less likely to overfit data than models with high descriptive complexity since there is less ‚Äúdescriptive space‚Äù in which to fit a lot of narrow/local rules in lower descriptive complexity models. As a result, if two models fit data/observations equally well but one has significantly lower descriptive complexity, it might make sense to select/use (or more heavily weight) the model with lower descriptive complexity.\n",
        "\n",
        "In order to penalize models with higher descriptive complexity in the training process, you need some way to measure a model‚Äôs descriptive complexity. In particular, you need to: (1) select a formal language to use to describe models, and (2) if there are multiple ways of describing a model in that language, you need to decide which of these descriptions should be used to measure the model‚Äôs descriptive complexity. A natural rule for (2) is to use the length of the shortest (i.e. the most ‚Äúcompressed‚Äù) description of the model (which is sometimes called the ‚ÄúKolmogorov complexity‚Äù of the model) as a measure of its descriptive complexity, since longer descriptions might contain redundant/extraneous/unnecessary symbols (e.g. the expressions f(x) = x and f(x) = x + 5 - 3 - 2 + 100 - 90 - 10 both describe the same function, but the second description has several unnecessary symbols). Unfortunately, however, it turns out that there is no general algorithm that can be used to find the length of the shortest description of a model. As a result, the best that can be done in practice is to use computable compression algorithms to try to approximate the descriptive complexity of a model. Lossless neural network compression techniques may therefore be useful when trying to approximate the descriptive complexity of different neural networks, which may in turn help to identify neural networks that have a better chance of generalizing well beyond the training data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9r4v5vng9ma"
      },
      "source": [
        "#### Q3: In this takehome, we will consider how singular value decomposition can be used to compress a deep neural network. Compared to other compression methods used for deep neural networks such as pruning, quantization, or efficient architectures, what are the relative merits/demerits of low rank approximations? Choose one or two alternative compression methods and compare with singular value decomposition.\n",
        "\n",
        "#### Answer:\n",
        "\n",
        "I‚Äôll compare low rank approximation to pruning and quantization. In particular, I‚Äôll compare the advantages and disadvantages of these approaches with respect to the characteristics of neural network inference that I identified in my response to the previous question (inference latency and throughput, computational costs and environmental impact, and quality of inferences/predictions). There are many different variations of pruning (e.g. you can prune any combination weights, neurons, or layers, you can reinitialize weights after after pruning and before retraining or you can retrain starting from the old weights, etc.) and quantization (e.g. you can replace 32 bit floating point numbers with either 16 bit floating point numbers or 8 bit integers, you can do quantization post-training or do ‚Äúquantization-aware‚Äù training, etc.). As a result, in addition to describing what I understand to be the general merits and demerits of these approaches relative to low rank approximation, I‚Äôll also try to describe how some of these variations might affect the advantages and disadvantages of these approaches. \n",
        "\n",
        "PRUNING VERSUS SINGULAR VALUE DECOMPOSITION\n",
        "\n",
        "Inference latency and throughput:\n",
        "\n",
        "-Both pruning and low-rank approximations can enable significant model compression. However, pruning that only prunes weights only enables compression if: (1) the pruning process results in sparse weight matrices, and (2) the sparse weight matrices are represented using specialized data structures (e.g. compressed sparse row) that exploit their sparsity to enable compression. However, the data structures used to compress sparse matrices are often not well-suited to exploiting the single instruction multiple data (SIMD) parallelism of GPUs when doing matrix multiplication. As a result, on GPUs sparse matrix multiplication algorithms sometimes run slower than dense matrix multiplication algorithms (which are very well-suited to exploiting the SIMD parallelism of GPUs) for matrices with comparable dimensions. So while weight pruning may enable significant model compression, in some cases GPU-based inference using the compressed representations of the weight matrices may not be faster (and may in fact be slower) than inference using the original weight matrices (though from what I understand this depends in part on how sparse the weight matrices are after pruning, with extremely sparse matrices sometimes enabling faster inference). As a result, pruning only weights may not actually result in a substantial improvement in inference latency or throughput. \n",
        "\n",
        "-In contrast, low-rank approximation enables you to achieve substantial (lossy) compression of a weight matrix without using data structures specialized for compressing sparse matrices. In other words, you can often represent the three matrices that result from a low-rank approximation as two-dimensional arrays and still achieve a substantial amount of compression (and if it seems advantageous to represent these matrices using data structures specialized for sparse matrices, this is still an option). Because dense matrix multiplication algorithms are better-suited to exploiting the SIMD parallelism of GPUs than sparse matrix multiplication algorithms, in some cases low-rank approximation techniques may enable a greater improvement in inference latency and throughput for the same level of compression versus weight-only pruning when inference is being done on GPUs.\n",
        "\n",
        "-However, pruning techniques that prune neurons or entire layers from a neural network will typically enable both model compression and improvements in inference latency and throughput, since elimination of a neuron reduces the size of the corresponding weight matrix (and eliminates the need to do the computations required to compute the output of that neuron) and elimination of a layer will eliminate an entire weight matrix (and the need to do the computations required to compute the outputs of all the neurons in that layers).  \n",
        "  \n",
        "\n",
        "Costs and environmental impact:\n",
        "\n",
        "-Both pruning and low-rank approximation techniques can enable substantial compression of a neural network, which can reduce the amount of persistent storage needed to store the neural network. It‚Äôs not obvious to me that one technique consistently enables more compression than the other, since in both cases the amount you are able to compress a network may depend to a significant degree on how much you are willing to trade off prediction quality/information loss against compression (though perhaps in some cases the accuracy-compression ‚Äúefficient frontier‚Äù you are able to achieve with pruning may be better than what‚Äôs achievable with low-rank approximation since pruning is more flexible, more targeted, and lends itself to retraining). \n",
        "\n",
        "-As mentioned above, in some cases pruning only weights can result in less computationally efficient inference for a given level of compression than low-rank approximation. In cases where that is true, low-rank approximation may enable lower computational costs and environmental impact than weight-only pruning.  \n",
        "\n",
        "\n",
        "Quality of inferences/predictions:\n",
        "\n",
        "-When using low-rank approximation to compress a weight matrix, for a given level of compression there is essentially only one way to approximate the original weight matrix. Another way of putting this is that for a weight matrix of rank k, there are only k ways of approximating this matrix via low-rank approximation (essentially the only choice you have is what you want the rank of the approximating matrix to be). In contrast, pruning allows you to choose any subset of the weights in a matrix to zero out, and also allows you to eliminate entire weight matrices or rows of weight matrices. Moreover, pruning allows you to target what exactly you prune based on estimates how much pruning something will impact the training loss (e.g. you can compute the partial derivative of the loss with respect to each weight parameter and then only eliminate weights where this partial derivative is small). The fact that pruning is more flexible than low-rank approximation (in the sense that there will typically be many more ways of achieving a given level of compression via pruning than via low-rank approximation) combined with the fact that pruning can be done in a targeted fashion suggests that pruning may in many cases enable higher quality inferences/predictions for a given level of compression than low-rank approximation.\n",
        "\n",
        "-In addition, pruning lends itself very naturally to retraining/fine-tuning of a pruned/compressed neural network. On the other hand, it is not obvious how to do fine-tuning/retraining on neural networks that have been compressed using singular value decomposition, since the retraining process might result in weight matrices with higher rank than the original low-rank approximations. If this happens, these higher-rank retrained weight matrices would then need to be approximated using singular value decomposition again to achieve the same level of compression. It is perhaps an interesting research question whether this cycle of low rank approximation‚Üíretraining‚Üílow rank approximation would produce compressed neural networks that make higher quality predictions than simply doing low rank approximation once, but it isn‚Äôt obvious to me that it would. The fact that pruning lends itself naturally to retraining/fine-tuning while low-rank approximation doesn‚Äôt further suggests that pruning may tend to enable higher quality inferences/predictions for a given level of compression than low-rank approximation. \n",
        "\n",
        "\n",
        "QUANTIZATION VERSUS SINGULAR VALUE DECOMPOSITION:\n",
        "\n",
        "Inference latency and throughput:\n",
        "\n",
        "-Both quantization and low-rank approximation can enable substantial compression of neural networks. In addition, the compressed neural network representations produced by both of these techniques are compatible with dense matrix multiplication algorithms that exploit the SIMD parallelism of GPUs. As a result, both techniques can enable substantial improvements in inference latency and throughput, since they can reduce the number of memory references required for inference in a way that doesn‚Äôt result in inefficient use of GPU resources (as can sometimes be the case with pruning). Quantization that converts floating point numbers to integers may enable further improvements in latency and throughput if the network inputs are also integers (or are converted from floating point to integer format before inference), since integer operations often take fewer cycles to execute than the corresponding floating point operations. With that said, it is not obvious to me that one technique consistently enables greater improvements in latency and throughput than the other, since in both cases the extent of these improvements may depend both on (1) the extent to which you‚Äôre willing to trade off latency and throughput against information loss, and (2) the architecture of the computer the inference is running on.  \n",
        "\n",
        "\n",
        "Costs and environmental impact:\n",
        "\n",
        "-As mentioned above, both quantization and low-rank approximation can enable substantial neural network compression, which can significantly reduce the amount of persistent storage required to store neural networks. In addition, since both techniques can also enable inference to be done with fewer memory references and using fewer compute cycles, both techniques can enable substantial reductions in computational costs and energy usage. It‚Äôs not obvious to me that one technique is consistently better than the other along these dimensions. \n",
        "\n",
        "\n",
        "Quality of inferences/predictions:\n",
        "\n",
        "-Post-training quantization is arguably slightly more of a blunt instrument than low-rank approximation, since post-training quantization indiscriminately reduces the precision of all network weights while low-rank decomposition enables you to find weight matrix approximations that are as ‚Äúclose‚Äù as possible to the original weight matrices for a given level of compression. As a result, you might expect that low-rank approximation will tend to enable higher quality inferences/predictions than quantization for a given level of compression (though admittedly the argument I‚Äôm making here isn‚Äôt totally rigorous and is based in part on my intuition). \n",
        "\n",
        "-Quantization-aware training allows you to search through the quantized parameter space for models that fit the training data well. As a result, quantization-aware training is arguably a more targeted way to compress models than post-training quantization and therefore may result in models that make higher quality predictions for a given level of compression. However, if the loss landscape in parameter space has sharp ‚Äúpeaks‚Äù and ‚Äúvalleys,‚Äù even quantization-aware training might not be able to find models that fit the training data well, if none of the areas of parameter space where training loss is low are on the ‚Äúgrid‚Äù of quantized parameters. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O81HH6D3Lugd"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoWy4-iQIYJ0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBFa6wXqAKgK",
        "outputId": "b9ab43c6-491d-460e-d950-b977750af453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dm-haiku\n",
            "  Downloading dm_haiku-0.0.8-py3-none-any.whl (350 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350 kB 16.4 MB/s \n",
            "\u001b[?25hCollecting optax\n",
            "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 145 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting jmp>=0.0.2\n",
            "  Downloading jmp-0.0.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (4.1.1)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.3.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.22+cuda11.cudnn805)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.23)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.5-py3-none-any.whl (85 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 85 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (0.8.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (3.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.1.55->optax) (5.10.0)\n",
            "Installing collected packages: jmp, chex, optax, dm-haiku\n",
            "Successfully installed chex-0.1.5 dm-haiku-0.0.8 jmp-0.0.2 optax-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install dm-haiku optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzdlf_milUbR"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, Mapping, Tuple\n",
        "from copy import deepcopy\n",
        "import time\n",
        "from absl import app\n",
        "import haiku as hk\n",
        "import matplotlib.pyplot as plt\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "import math\n",
        "import sys\n",
        "\n",
        "Batch = Tuple[np.ndarray, np.ndarray] # Defining an alias for a type that is a tuple containing two NumPy arrays, presumably for use in type hints for a static analysis tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsYJmUqz-uFT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjTGxEfqnSmd"
      },
      "source": [
        "## Coding Challenge Part I : Debugging Challenge (10 Points)\n",
        "\n",
        "\n",
        "We are now going to explore using SVD to compute low rank approximations of the parameters of a small deep neural network. You are using a very simple toy model as a first baseline. Section 3 will give you the chance to improve baseline accuracy beyond this very simple model -- this is just a toy setting to first explore low rank approximations.\n",
        "\n",
        "The first part of this challenge is primarily a debugging challenge. It will require removing bugs in order to train a very simple network. We have introduced several bugs -- some are subtle and will not break your code but will degrade final performance. These subtle bugs are introduced to understand your grasp of fundamental machine learning principles. There are also more obvious bugs designed to break your code. \n",
        "\n",
        "* [**4 points**] Your goal is to get the code working. There are 4 bugs in the code, these are subtle bugs which are designed to impair test accuracy but not break the code. You will get partial points for each of the 4 bugs you find. After finding all bugs, your test performance should be around 66-67% test accuracy.\n",
        "\n",
        "* [**2 points**] We will give extra points for also adding improved documentation to each of the functions we introduce in this section, and for describing the fixes to the bugs. \n",
        "\n",
        "* [**4 points**] There are also two functions you will need to code up in this section -- we indicate where these code changes need to happen with TODO comments. \n",
        "\n",
        "* Do not alter the model architecture or the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqpgWQJs-evw"
      },
      "outputs": [],
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465) # mean values for the red, green, and blue channels respectively for all of the pixels in all of the images in the normalized CIFAR10 training dataset\n",
        "\"\"\"\n",
        "RAVI'S COMMENT: In the cell below the \"Training\" cell, there's code that \n",
        "calculates the mean and standard deviation for each channel for the full \n",
        "CIFAR10 training set. The results of that code indicate that the standard \n",
        "deviation values in the original version of this notebook are incorrect,\n",
        "so below I commented the original line out and assigned the correct values \n",
        "to the CIFAR10_STD variable instead.  \n",
        "\"\"\"\n",
        "# CIFAR10_STD = (0.2023, 0.1994, 0.2010) \n",
        "CIFAR10_STD = (0.2472, 0.2437, 0.2617) # standard deviations of the values for the red, green, and blue channels respectively for all of the pixels in all of the images in the normalized CIFAR10 training dataset\n",
        "\n",
        "\"\"\"\n",
        "The function below defines the convolutional neural network architecture that \n",
        "is used for this take-home challenge.  \n",
        "\"\"\"\n",
        "def net_fn(batch: Batch) -> jnp.ndarray:\n",
        "\n",
        "  x = normalize(batch[0]) # normalization and standardization of image data before it is input into neural network\n",
        "  \n",
        "  # Do NOT alter the architecture definition below.\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)\n",
        "\n",
        "def load_dataset(\n",
        "    split: str,\n",
        "    *,\n",
        "    is_training: bool,\n",
        "    batch_size: int,\n",
        ") -> Iterator[tuple]:\n",
        "  \"\"\"Loads the dataset as a generator of batches.\"\"\"\n",
        "  ds = tfds.load('cifar10', split=split, as_supervised=True).cache().repeat() # Load the specificied split of the CIFAR10 dataset\n",
        "  if is_training: # If loaded data is for training, shuffle data each epoch to randomize the samples used for SGD.  \n",
        "    ds = ds.shuffle(10 * batch_size, seed=0) \n",
        "  ds = ds.batch(batch_size) # Combine consecutive elements of the dataset into batches of size batch_size\n",
        "  return iter(tfds.as_numpy(ds)) # Convert the dataset to an iterable of NumPy arrays and return\n",
        "\n",
        "\"\"\"\n",
        "RAVI'S COMMENT: I think it might be better if the model function is passed to compute_loss as an argument\n",
        "rather than the body of the function referencing the global variable \"net.\" This would probably make this \n",
        "function more easy to reuse if you want to try out different architectures (so that you don't have to create\n",
        "a separate function for each architecture, or always reassign net each time you want to use this function \n",
        "with a new architecture).\n",
        "\"\"\"\n",
        "def compute_loss(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  \"\"\"Compute the loss of the network, including L2.\"\"\"\n",
        "  x, y = batch # Assign image data to x variable and label data to y variable\n",
        "  logits = net.apply(params, batch) # Compute logits for each example in batch\n",
        "  labels = jax.nn.one_hot(y, 10) # One-hot encode true labels\n",
        "\n",
        "  # TODO: add code below to compute the l2_loss variable\n",
        "  \n",
        "  \"\"\"\n",
        "  RAVI's COMMENT: I think it's confusing to call the below variable \"l2_loss.\" The rest of the code in this function\n",
        "  suggests that this variable is actually intended to be used for L2 regularization and is not in fact a loss,\n",
        "  so I think it would be clearer to name it \"l2_regularization\" or something like that.\n",
        "  \"\"\"\n",
        "  \n",
        "  squared_params = jax.tree_util.tree_map(lambda x: x ** 2, params) # Squaring each parameter in the params pytree\n",
        "  l2_loss = sum(jnp.sum(p) for p in jax.tree_util.tree_leaves(squared_params)) # Summing up all of the squared parameters to calculate the L2 regularization term\n",
        "\n",
        "  \"\"\"\n",
        "  RAVI's COMMENT: Not clear what the purpose of the line below is, since weighted_l2_loss isn't used again. I suggest deleting it. \n",
        "  More generally, my understanding is that \"weighted loss\" typicaly refers to a loss function that weights the loss of different \n",
        "  observations differently depending on characteristics of the observation (e.g. because it may be more important to make accurate \n",
        "  predictions for certain types of examples than others). But, as mentioned above, it doesn't seem like \"l2_loss\"\n",
        "  is even supposed to be a true loss value, so it's unclear what the point of the weighted_l2_loss variable is.\n",
        "  \"\"\"\n",
        "  \n",
        "  weighted_l2_loss = 0.5 * l2_loss  \n",
        "  \n",
        "  softmax_xent = -jnp.sum(labels * jax.nn.log_softmax(logits)) # Compute the negative cross-entropy/log-likelihood of the batch for the given params, assuming probabilities are calculated by applying the softmax function to the vector of logits \n",
        "\n",
        "  \"\"\"\n",
        "  RAVI'S COMMENT: The original code for the line below subtracted the second term from the first term. I changed the code\n",
        "  so that the second term is added to the first term. Since softmax_xent is the negative cross-entropy/log likelihood\n",
        "  of the batch observations, it makes sense to add the regularizing term (so that more \"complex\" parameters are penalized),\n",
        "  not subtract it. Of course, you could achieve the same effect by putting a negative sign at the beginning of the right \n",
        "  hand side of the l2_loss assignment, but adding the second term in the line below is more natural.\n",
        "  \"\"\"\n",
        "  \n",
        "  return softmax_xent + (1e-4 * l2_loss) # Calculate and return the cross-entropy loss with L2 regularization \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The function below computes the accuracy of the network with parameters given by the \"params\"\n",
        "variable on the dataset specified by the \"batch\" variable. \n",
        "\n",
        "RAVI'S COMMENT: It would probably be better if compute_accuracy was defined so that\n",
        "the model function is passed to it as an argument as opposed to referenced as a global\n",
        "variable. That way, the compute_accuracy function could be reused for different\n",
        "model architectures rather than needing to create a new compute_accuracy function\n",
        "for each model architecture you want to evaluate (or reassigning the \"net\" variable\n",
        "to be equal to different model function). \n",
        "\"\"\"\n",
        "@jax.jit\n",
        "def compute_accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  predictions = net.apply(params, batch) # Compute logits for each example in batch\n",
        "\n",
        "  # TODO: add code below to compute the accuracy over the batch.\n",
        " \n",
        "  \"\"\"\n",
        "  The line below computes accuracy by: (1) calculating the index of the column with \n",
        "  the highest logit value for each prediction (which corresponds to the number of \n",
        "  the CIFAR10 category that the image is most likely to fall into, according to the\n",
        "  prediction), (2) comparing the index from (1) to the \"true\" category number, and\n",
        "  (3) calculating the fraction of predictions that are correct.\n",
        "  \"\"\"\n",
        "  accuracy = jnp.mean(jnp.argmax(predictions, axis=1) == batch[1])\n",
        "  return accuracy\n",
        "\n",
        "\"\"\"\n",
        "The function below executes a round of gradient-based updates to the parameters of \n",
        "a model. \n",
        "\"\"\"\n",
        "@jax.jit\n",
        "def update(\n",
        "    params: hk.Params,\n",
        "    opt_state: optax.OptState,\n",
        "    batch: Batch,\n",
        ") -> Tuple[hk.Params, optax.OptState]:\n",
        "  grads = jax.grad(compute_loss)(params, batch) # Compute the gradient of the loss function for the given batch with respect to each parameter\n",
        "  updates, opt_state = opt.update(grads, opt_state) # Compute the parameter updates and the new optimizer state\n",
        "  new_params = optax.apply_updates(params, updates) # Apply the parameter updates to produce new parameter values\n",
        "  return new_params, opt_state\n",
        "\n",
        "\"\"\"\n",
        "The function below calculates and returns the Polyak average of \n",
        "all of the parameter assignments that have been tried up to a certain point. \n",
        "avg_params is the Polyak average up to just before the most recent parameter update,\n",
        "params is the parameter assignment after the most recent update, and step_size\n",
        "determines how much avg_params and params are weighted when calculating the \n",
        "average (i.e. how fast the weight assigned to past parameter assignments decays\n",
        "after each new iteration). \n",
        "\"\"\"\n",
        "@jax.jit\n",
        "def ema_update(params, avg_params):\n",
        "  return optax.incremental_update(params, avg_params, step_size=0.001)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The function below both normalizes and standardizes CIFAR10 \n",
        "image data. \n",
        "\n",
        "RAVI'S COMMENT: It may be clearer/more accurate to name this function  \n",
        "\"normalize_and_standardize\" or something like that, since the current \n",
        "name suggests it only normalizes the input data when in fact it both \n",
        "normalizes and standardizes the input data.  \n",
        "\"\"\"\n",
        "def normalize(images):\n",
        "  mean = np.asarray(CIFAR10_MEAN) # converting CIFAR10_MEAN list into a NumPy array\n",
        "  std = np.asarray(CIFAR10_STD) # converting CIFAR10_STD list into a NumPy array\n",
        "  \n",
        "  \"\"\"\n",
        "  The line below originally cast the images array to jnp.int8 type. This seemed\n",
        "  like it caused some of the normalized pixel-channel values to become negative, \n",
        "  which indicated something went wrong because the normalized values should all \n",
        "  be between 0 and 1. I suspect that the problem is that the original CIFAR10 \n",
        "  image data is in np.uint8 type, and when casting to jnp.int8 some of the \n",
        "  unsigned integers are interpreted as having negative values depending on the \n",
        "  value of the most significant/sign bit. In any case, the int8 format does not \n",
        "  even have enough non-sign bits to represent all of the positive integers in \n",
        "  the range from 0 to 255 (i.e. the range of possible pixel-channel values in \n",
        "  the CIFAR10 data). As a result of the stuff described above, I changed the line\n",
        "  to cast the data to jnp.uint8 instead of jnp.int8.    \n",
        "  \"\"\"\n",
        "  x = images.astype(jnp.uint8) / 255. # Cast the images array to jnp.uint8 and normalize \n",
        " \n",
        "\n",
        "  \"\"\"\n",
        "  The purpose of the two lines below is to standardize the CIFAR10 input data. \n",
        "  The first line originally read \"x /- mean\". I believe that this would divide\n",
        "  pixel values for each color by the negative of the mean for that color, which\n",
        "  isn't what we want to do (it also seems like the result of this computation \n",
        "  wasn't assigned to any variable, so it wouldn't have any impact on the rest of\n",
        "  the program). Instead, I changed the line below so that it subtracts the mean \n",
        "  values for each color from the corresponding pixel values and assigns the \n",
        "  result to x. \n",
        "  \"\"\"\n",
        "  x -= mean \n",
        "  x /= std\n",
        "\n",
        "  return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5iI930cIjzM"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "a008cf05978447c488da8d273be40900",
            "d956211357364f28b13304d2a389438c",
            "6cfac5d568814b11bc6b9b05aa0f2e40",
            "60bc3ed4c92948328ad28220d2b890aa",
            "415f3f66e6c44a3f96b503309d136e8f",
            "20178df3a17443d4a4d9716db32fd864",
            "39b4cb5ba9594f309bcac087b8d9cf51",
            "3ad0702206644bcfadab9b129fb8aa85",
            "30165e701353434a97cb9acff4ec2c46",
            "9be3b4f925d44ccf97adb231362a6ae3",
            "c87965aab61f4ae7aa9f88e56c507ba0",
            "1923b659376b4db1b143e702469c7496",
            "2d1b8f5c7e2c4e88973be10585249a68",
            "9bbb7e12070041ffbe7f1cb5e71d551f",
            "adf024f3ad3846baafffbfb5af72e396",
            "68b69e94a7fd4a8caff8794c7a864ae8",
            "275b17134b6e4ac0a77bb4234403dfb2",
            "f4d697c6703b4c76b9262043bd715203",
            "c4fb078a5e3c461bad5e4dba53291b3f",
            "56cef49d8a1c494ba7f84832a6ddd264",
            "fbc91c9b31794b2eb8cd722be6d03608",
            "c23bd2e2768c46ce8eebf13da2900fb4",
            "57a011ad7492448fab9978965ce775ae",
            "f7d3f1b8927940c3a589ec69a08716fe",
            "7f2ed601d0e541aca8573babaa719900",
            "134658d4cd324e5b92469d7c643427dc",
            "e43d4f807dbe43fdb0c5667159b66cec",
            "aee34abfcfa94145ab86dbec3874f2d8",
            "50a77f27cb1f4f14aa978fabfdafa5f9",
            "009d67202aef44d9a4e623ad124d1e31",
            "78cbd879a42c4bf587b3185f55e7a504",
            "d4e3ae0285b8498fb54ec8dfcb4f45b9",
            "fbf5e8399e8c489da4298d55da89088f",
            "97405f587fc6488e94872c927fc3655e",
            "465d5ba9ae8e400f8ed8c386961ef4d2",
            "2be28ed8a87f4c1c9e26ac177f1958d4",
            "9de69f8763ca44a0bca423693e1eb6fa",
            "d0c8763260164b69b5ca49b21f2b9ee4",
            "4bc62f7a876a4ca5bd60c5c8b545dbe8",
            "83bc1a5a2d0842bd9184f194e575ec26",
            "9d148e98c91f49f9986455ede7dcb70b",
            "6a4128bab3f94fd39b86ce0b8b6393bf",
            "a7dff65e7cdc42b7a2a040d50949586f",
            "c5eaa8de99954f6a96d51d233b13af7b",
            "a5d429ebddba4bd8b0988d382e55fca0",
            "ca10413c6c384941888f6cc88da6b676",
            "0246e59074cf4a53a2be5b3de81d38d4",
            "a06489bfb9e04edf8f7521a0963f8c03",
            "8fbd45dc85fb44f489e0828142aa00ea",
            "9bbae03d22c048148db3d4f1aa7a771e",
            "8192f6ba122442b9aa2eee9f73f6981a",
            "bfe66aae46894809ba902e7a1f2da6aa",
            "b5dacc67acdb426f8c0ea29ecad0343e",
            "e6e9ebca1e0e4c2f91cbf479460a2ebb",
            "367a5729bb3146b7ae84604fcbd8254e",
            "7ff2faaaa36d4067944d7ff4fbdf8349",
            "3288ff2bbedf455fbffe21434c113848",
            "4193de256a3848f38088f83dcddc972d",
            "eb754e8fd8de4a22aacc93c7d27b0d18",
            "9378006e425e452381e3dd12c78e3d5d",
            "972bef0331e344ecbfafc3f0746a5d3a",
            "8422e3485b0a4f659ab7d2a5d3ae28f2",
            "957eae27b93d4e678c45605b7290d5d6",
            "7b3a22f9cce54e3bbaa3fd91bc16f46f",
            "364f1fbc51dd4a0397a9ee051e9f7aa6",
            "4603e2cb0cd84ccd81b513c5d21795ec",
            "e8a19fae63a842ca8d96efee0dc57b56",
            "9ceb056b8f034355ac6a1fd06a0427f8",
            "379c566312d342cdbe88dd05786a9de0",
            "032597a75e484d88877019227b4f0a45",
            "109506268cbb4d189e5f3211cbee219a",
            "9402317f9f3147d7847891559e303474",
            "b63eab95ce0b416d98619b6d2fc35ed7",
            "d42e6ed4514d4b579b01d3b1a351ce3f",
            "78c82749f85a4503a5b0b1e9361b7be4",
            "f08ee4e788ca4fa2acaa771022b7bc5a",
            "d8b0c53da29a40a0a32a651a845c6ff6",
            "b004acfe68f548d2a3122af09fa6fea0",
            "a7421529bc294d10a9c7a40aa04bcab9",
            "f7b82acda00c4b7481fc449bc8b55ba0",
            "943a80a684ea49578bfa9f823f0a2701",
            "f60575b314874b47bbe046362b1fdda5",
            "d760140394a74285aeac403b6f6d9241",
            "63bd73d14003497ca828a3af175b22ec",
            "45e0c193d8054ec688957807ce41b41e",
            "da89e20930a54612a5926b49d80526b1",
            "fcbe4de60a49493e988076c05e3cc0fb",
            "2339a1dce506453981ed84f44d317b60"
          ]
        },
        "id": "Q4-HuMSH_Cbw",
        "outputId": "8622ff7f-297c-4acc-8618-7ae8d946ba4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 162.17 MiB (download: 162.17 MiB, generated: 132.40 MiB, total: 294.58 MiB) to ~/tensorflow_datasets/cifar10/3.0.2...\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a008cf05978447c488da8d273be40900"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1923b659376b4db1b143e702469c7496"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extraction completed...: 0 file [00:00, ? file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57a011ad7492448fab9978965ce775ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97405f587fc6488e94872c927fc3655e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5d429ebddba4bd8b0988d382e55fca0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteTDR4PH/cifar10-train.tfrecord*...:   0%|          | 0/‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ff2faaaa36d4067944d7ff4fbdf8349"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8a19fae63a842ca8d96efee0dc57b56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteTDR4PH/cifar10-test.tfrecord*...:   0%|          | 0/1‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b004acfe68f548d2a3122af09fa6fea0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDataset cifar10 downloaded and prepared to ~/tensorflow_datasets/cifar10/3.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Step 0] Test accuracy: 0.103.\n",
            "[Step 1000] Test accuracy: 0.422.\n",
            "[Step 2000] Test accuracy: 0.572.\n",
            "[Step 3000] Test accuracy: 0.631.\n",
            "[Step 4000] Test accuracy: 0.659.\n",
            "[Step 5000] Test accuracy: 0.673.\n",
            "[Step 6000] Test accuracy: 0.678.\n",
            "[Step 7000] Test accuracy: 0.677.\n",
            "[Step 8000] Test accuracy: 0.678.\n",
            "[Step 9000] Test accuracy: 0.672.\n",
            "[Step 10000] Test accuracy: 0.669.\n"
          ]
        }
      ],
      "source": [
        "net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "\n",
        "# Do not change learning rate\n",
        "opt = optax.adam(1e-3) # Create a GradientTransformation object for an Adam optimzier\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "RAVI'S COMMENT: The three lines below originally divided up the CIFAR10 data into \n",
        "a training set, a validation set, and a test set. The data that the people who \n",
        "developed the CIFAR10 dataset call training data was split into training and \n",
        "validation sets by the lines below, with 80% of the data allocated to the validation\n",
        "set and 20% allocated to the training set. This way of breaking up the CIFAR10 data \n",
        "didn't make sense to me for a couple reasons. First, it's not clear what the point\n",
        "of having both a validation set and a test set is for this exercise. My understanding\n",
        "is that validation sets are typically used to aid with hyperparameter/architecture \n",
        "evaluations and selection. But given that the instructions for the take-home \n",
        "explicitly say that the model architecture should not be changed, it seems like there's\n",
        "no need for a validation set. Moreover, even if we needed to use a validation set for some\n",
        "purpose, it would be unusual to have four times the amount of data in the validation set\n",
        "as in the training set (my understanding is that typically significantly more data is \n",
        "allocated to the training set versus the validation set). As a result, I commented out\n",
        "the line that creates the validation set, and I changed the line creating the training\n",
        "set so that all of the data in the \"train\" split of the CIFAR10 data is allocated to\n",
        "that set. This will result in much more data being used for tuning the model parameters, \n",
        "which will hopefully help to improve performance.\n",
        "\n",
        "I also changed the minibatch size for the training set from 1000 to 32. From what I can \n",
        "tell, it's not common to use a minibatch size as large as 1000 and it seems like people \n",
        "have found empirically that smaller minibatch sizes often lead to better generalization.\n",
        "One hypothesis about why this is the case is that using smaller minibatches introduces \n",
        "noise into the estimate of the loss gradient, which may result in more \"exploration\"\n",
        "of parameter space and therefore make it more likely that the optimization process \n",
        "finds and converges on a better local optimum than if it used a less noisy gradient\n",
        "estimate (in which case it might quickly converge on a local optimum that is \"near\" \n",
        "the initial parameters). More broadly, introducing randomness/noise into gradient-based\n",
        "local search is a common technique used to avoid converging to undesirable local optima,\n",
        "and using a smaller minibatch size is arguably an instance of this broader technique applied to\n",
        "learning. My understanding is that 32 is a minibatch size that is commonly used and empirically\n",
        "often seems to work well (presumably 32 is used as opposed to other numbers close to it\n",
        "in order to optimally exploit the degree of parallelism available in GPUs and TPUs).     \n",
        "\"\"\"\n",
        "\n",
        "train = load_dataset(\"train\", is_training=True, batch_size=32)\n",
        "# validation = load_dataset(\"train[0:80%]\", is_training=False, batch_size=10000)\n",
        "test = load_dataset(\"test\", is_training=False, batch_size=10000)\n",
        "\n",
        "\n",
        "params = avg_params = net.init(jax.random.PRNGKey(42), next(train)) # Initialize model parameters\n",
        "opt_state = opt.init(params) # Initialize state of the Adam optimizer\n",
        "\n",
        "\n",
        "for step in range(10001):\n",
        "  if step % 1000 == 0:\n",
        "    \n",
        "    \"\"\"\n",
        "    RAVI'S COMMENT: Since I eliminated the validation set above, I commented\n",
        "    out the line that calculates accuracy on batches of the validation set.  \n",
        "    \"\"\"\n",
        "    \n",
        "    # val_accuracy = compute_accuracy(avg_params, next(validation))\n",
        "    test_accuracy = compute_accuracy(avg_params, next(test))\n",
        "    \n",
        "   \n",
        "    \n",
        "    \"\"\"\n",
        "    The below line transfers the value of test_accuracy from hardware accelerator \n",
        "    memory to host memory so it can be printed.\n",
        "    \"\"\"\n",
        "    test_accuracy = jax.device_get(test_accuracy) \n",
        "    \n",
        "    print(f\"[Step {step}] Test accuracy: {test_accuracy:.3f}.\") # Print out test accuracy\n",
        "\n",
        "  params, opt_state = update(params, opt_state, next(train)) # Execute another iteration of parameter optimization\n",
        "  avg_params = ema_update(params, avg_params) # Compute the updated Polyak average of all the parameter assignments\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBWQhbV-MPQA"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "RAVI'S COMMENT: The code in this cell is calculates the mean and standard deviation\n",
        "for the red, green, and blue channels for the CIFAR10 training data set. The purpose \n",
        "of doing this is to verify that the values in the CIFAR10_MEAN and CIFAR10_STD variables\n",
        "are correct. It seems like the original values for CIFAR10_MEAN are correct but the \n",
        "values for CIFAR10_STD are not (when they are used to standardize the full CIFAR10 training\n",
        "dataset, the standard deviation for each channel for the standardized dataset is not 1).\n",
        "As a result, I changed the CIFAR10_STD values to the correct values.\n",
        "\"\"\"\n",
        "\n",
        "train_full = load_dataset(\"train\", is_training=True, batch_size=50000) # Load the CIFAR10 training set with batch size equal to size of the whole dataset\n",
        "norm_training_images = next(train_full)[0]/255 # Normalize CIFAR10 training set image data and assign to a variable\n",
        "norm_training_images_mean = jnp.mean(norm_training_images, axis = [0,1,2]) # Calculate means for each color\n",
        "norm_training_images_std = jnp.std(norm_training_images, axis = [0,1,2]) # Calculate standard deviations for each color\n",
        "\n",
        "print(f\"color means of normalized CIFAR10 training data: {norm_training_images_mean}\") # Print means\n",
        "print(f\"color standard deviations of normalized CIFAR10 training data: {norm_training_images_std}\") # Print standard deviation\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUSgUHY-pMm8",
        "outputId": "6cdcfa0b-828d-4729-e95f-1922c41658a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "color means of normalized CIFAR10 training data: [0.49171057 0.48227102 0.44622934]\n",
            "color standard deviations of normalized CIFAR10 training data: [0.24721377 0.243744   0.26172954]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQmOspeD0jCh"
      },
      "source": [
        "## Coding Challenge Part 2: Compression through Low Rank Approximation (8 points)\n",
        "\n",
        "In this section, you will add code to compute the low rank approximation and to compute evaluation metrics. We will evaluate whether the low rank approximation allows for speed up in inference time. We define inference time as the average time to compute the prediction for all examples in the test set.\n",
        "\n",
        "* [**4 points**] You will need to add code to define both the compute_eval_metrics and rank_approximated weight function. \n",
        "* [**4 points**] Q4 and Q5 are worth 2 points each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdZqO7W_KSX8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The function below computes a list of latencies and a list of accuracies for the\n",
        "input parameters and batch.\n",
        "\n",
        "RAVI'S COMMENT: It would probably be better if compute_eval_metrics was defined so that\n",
        "the model function is passed to it as an argument as opposed to referenced as a global\n",
        "variable. That way, the compute_eval_metrics function could be reused for different\n",
        "model architectures rather than needing to create a new compute_eval_metrics function\n",
        "for each model architecture you want to evaluate (or reassigning the \"net\" variable to \n",
        "a new function). \n",
        "\"\"\"\n",
        "def compute_eval_metrics(params, batch, n_samples):\n",
        "# TODO: add code to compute the time for inference.\n",
        "  duration_list = []\n",
        "  accuracy_list = []\n",
        "  for _ in range(n_samples):\n",
        "    start = time.perf_counter() # Get the time immediately before running inference\n",
        "    predictions = net.apply(params, batch) # Compute logits for each example in batch\n",
        "    duration = time.perf_counter() - start # Get the time immediately after running inference and subtract from start time to get an estimate of inference latency\n",
        "    acc = compute_accuracy(params, batch) # Compute accuracy for batch. \n",
        "    duration_list.append(duration) # Append most recent latency measurement to list of latency measurements\n",
        "    accuracy_list.append(acc) # Append most recent accuracy measurement to list of accuracy measurements\n",
        "\n",
        "  return accuracy_list, duration_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8U8Nlp9IS4q"
      },
      "outputs": [],
      "source": [
        "def rank_approximated_weight(weight: jnp.ndarray, rank_fraction: float):\n",
        "  # TODO: replace the code below with code to compute the SVD of the matrix to return the rank approximated weights u and v for a given matrix.\n",
        "  weight_rank = min(weight.shape) # Calculate the rank (or rather the maximum possible rank) of the weight matrix\n",
        "  reduced_rank = int(rank_fraction * weight_rank) # Calculate rank of low rank approximation to weight matrix \n",
        "  \n",
        "  \"\"\"\n",
        "  RAVI'S COMMENT: In my opinion it would be clearer to call the variable v\n",
        "  that is returned from this function vt or v_t since it seems like in the \n",
        "  normal notation for singular value decomposition that matrix is the transpose \n",
        "  of the matrix V whose columns are the right singular vectors.\n",
        "  \"\"\"\n",
        "  u, s, v = jnp.linalg.svd(weight) # Compute singular value decomposition of weight matrix\n",
        "  \n",
        "  \"\"\"\n",
        "  RAVI'S COMMENT: In my opinion it would be clearer to call the variable u that \n",
        "  is returned from this function something like u_s since it is actually a truncated \n",
        "  version of u multiplied by a truncated version of the matrix of singular values.\n",
        "  \"\"\"\n",
        "  u = u[:,:reduced_rank] @ jnp.diag(s)[:reduced_rank,:reduced_rank] # Calculate the product of the left matrix and singular value matrix of the SVD, where both matrices are appropriately truncated given the value of reduced_rank\n",
        "  v = v[:reduced_rank,:] # Truncate the right matrix of the SVD based on the value of reduced_rank\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  return u, v"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvm9InR4JG4F"
      },
      "source": [
        "### Evaluations at different ranks\n",
        "\n",
        "The code below first replaces the weights with the low rank factorizations at different rank fractions. For each modified net, we compute the new eval accuracy. Firstly, add code for the rank_approximated_weight and add code to correctly compute the time for inference (the duration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEjLxaDPEGEY",
        "outputId": "e68888b7-83ac-489a-daf0-6c8be4b92c1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model at 1.0\n",
            "Rank Fraction / Test accuracy: 1.00 / 0.626.\n",
            "Rank Fraction / Duration: 1.00 / 0.6705.\n",
            "Evaluating the model at 0.9\n",
            "Rank Fraction / Test accuracy: 0.90 / 0.625.\n",
            "Rank Fraction / Duration: 0.90 / 0.5149.\n",
            "Evaluating the model at 0.8\n",
            "Rank Fraction / Test accuracy: 0.80 / 0.626.\n",
            "Rank Fraction / Duration: 0.80 / 0.5252.\n",
            "Evaluating the model at 0.7000000000000001\n",
            "Rank Fraction / Test accuracy: 0.70 / 0.625.\n",
            "Rank Fraction / Duration: 0.70 / 0.5232.\n",
            "Evaluating the model at 0.6000000000000001\n",
            "Rank Fraction / Test accuracy: 0.60 / 0.598.\n",
            "Rank Fraction / Duration: 0.60 / 0.5218.\n",
            "Evaluating the model at 0.5000000000000001\n",
            "Rank Fraction / Test accuracy: 0.50 / 0.557.\n",
            "Rank Fraction / Duration: 0.50 / 0.5226.\n",
            "Evaluating the model at 0.40000000000000013\n",
            "Rank Fraction / Test accuracy: 0.40 / 0.507.\n",
            "Rank Fraction / Duration: 0.40 / 0.5202.\n",
            "Evaluating the model at 0.30000000000000016\n",
            "Rank Fraction / Test accuracy: 0.30 / 0.435.\n",
            "Rank Fraction / Duration: 0.30 / 0.5209.\n",
            "Evaluating the model at 0.20000000000000018\n",
            "Rank Fraction / Test accuracy: 0.20 / 0.357.\n",
            "Rank Fraction / Duration: 0.20 / 0.5226.\n",
            "Evaluating the model at 0.1000000000000002\n",
            "Rank Fraction / Test accuracy: 0.10 / 0.191.\n",
            "Rank Fraction / Duration: 0.10 / 0.5239.\n"
          ]
        }
      ],
      "source": [
        "rank_truncated_params = deepcopy(params)\n",
        "ranks_and_accuracies = []\n",
        "ranks_and_times = []\n",
        "for rank_fraction in np.arange(1.0, 0.0, -0.1): \n",
        "\n",
        "  print(f\"Evaluating the model at {rank_fraction}\")\n",
        "  for layer in params.keys():\n",
        "    if 'conv' in layer: # Skip weight tensors of convolutional layers (i.e. this code only does low-rank approximation of weight matrices in the linear layers of the network). This probably won't have much an impact on inference time since there are orders of magnitude fewer weights for the convolutional layers versus the linear layers. \n",
        "      continue\n",
        "    weight = params[layer]['w'] # Assign the weight matrix for a linear layer to the \"weight\" variable\n",
        "    # TODO: complete coding the rank_approximated_weight function to compute the SVD of the matrix to return the rank approximated weights u and v for a given matrix.\n",
        "    u, v = rank_approximated_weight(weight, rank_fraction)\n",
        "    rank_truncated_params[layer]['w'] = u@v # Insert the low rank approximation of the weight matrix into the appropriate part of the parameter pytree. \n",
        "\n",
        "  test_batch = next(test)\n",
        "  # we compute metrics over 50 samples to reduce noise in the measurement.\n",
        "  n_samples = 50\n",
        "  # TODO: complete coding the compute_eval_metrics function to compute the time taken for inference.\n",
        "  \n",
        "  \"\"\"\n",
        "  RAVI'S COMMENT: As I note in my response to question 5 below, it seems like the first time this \n",
        "  evaluation code is run the latency for the network using the full rank weight matrices is significantly\n",
        "  higher than the latency for all of the networks using the lower rank approximations. As I describe\n",
        "  in more detail below, I suspect this may be simply due to the fact that the network with the full rank \n",
        "  matrices is run first. As a result, it may make sense to add an \"if\" statement above the line below that\n",
        "  runs compute_eval_metrics once if rank = 1.0 without assigning the results to any variables, so that \n",
        "  any overhead associated with fetching code/data from memory that is reused for subsequent inferences\n",
        "  is done before measuring latency. \n",
        "  \"\"\"\n",
        "  test_accuracy, latency = compute_eval_metrics(rank_truncated_params, next(test), n_samples)\n",
        "  print(f\"Rank Fraction / Test accuracy: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(test_accuracy):.3f}.\")\n",
        "  ranks_and_accuracies.append((rank_fraction, np.mean(test_accuracy)))\n",
        "  print(f\"Rank Fraction / Duration: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(latency):.4f}.\")\n",
        "  ranks_and_times.append((rank_fraction, np.mean(latency)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzyvqIr38eWw"
      },
      "source": [
        "### Q4: What do you observe as the relationship between rank fraction and test accuracy?\n",
        "\n",
        "Plot this relationship showing accuracy (y-axis) vs rank percentage of the matrix (x-axis). You should use the ranks_and_accuracies list computed above.\n",
        "\n",
        "Answer: Test accuracy tends to decrease as rank fraction decreases. This makes sense, since as rank fraction decreases the low rank approximations of the linear layer weight matrices will get \"farther\" (as measured by the Frobenius norm) away from the original trained weight matrices (i.e. as rank fraction decreases, the low rank approximations to the trained weight matrices become more crude). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TCdYJ6lSEKM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "6a6eede4-1eae-4464-87b9-7fd1adb5b3d2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "## TODO: add your code below to plot the relationship between time and test set accuracy\n",
        "\n",
        "ranks = [x[0] for x in ranks_and_accuracies] # Extract the ranks from the ranks_and_accuracies list of tuples\n",
        "accuracies = [x[1] for x in ranks_and_accuracies] # Extract the accuracies from the ranks_and_accuracies list of tuples\n",
        "plt.plot(ranks, accuracies) # Create plot of ranks versus accuracy\n",
        "plt.xlabel('rank fraction') # Label the axes\n",
        "plt.ylabel('accuracy') # Label the axes \n",
        "plt.show() # Display the plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRkI8rAO5UYe"
      },
      "source": [
        "### Q5: Does replacing the weight matrix with the low factor matrix result in latency speed ups?\n",
        "\n",
        "Plot the relationship of time (y-axis) vs rank percentage (x-axis). To do so add code to compute the ranks_and_times list.\n",
        "\n",
        "Answer: No, replacing the linear layer weight matrices with low rank approximations does not seem to reduce latency. This is not surprising, since the code is simply replacing the original linear layer weight matrices with low rank approximations to those matrices that are the same size/dimension. In other words, the code is not actually using a compressed representation of the linear layer weight matrices, so we would not expect a reduction in latency (since the number of memory references and arithmetic operations required to execute inference will be approximately the same for the network with the original weight matrices and the network with the low rank approximations). \n",
        "\n",
        "One caveat to note is that the first time the evaluation code is run, the latency for the network using the full weight matrices appears to be significantly higher than the latency for all of the networks using the low rank approximations (but the latency appears to be roughly the same for all of the networks low rank approximations). I suspect this is due to the fact that the network with the full rank weight matrices is run first, and the first time this network is used for inference there may be significant overhead associated with fetching code/data needed to run the network other than the linear layer weights (e.g. code/data related to the network architecture, the convolutional layer weights, etc.) from lower levels of the memory hierarchy. This overhead may then be significantly reduced for all subsequent inferences that reuse some of this code/data (since the code/data may already be in higher levels of the memory hierarchy and therefore may be much faster to fetch), which would include all of the evaluations of the lower rank approximations. Notably the difference in latency between the network that uses the full rank weight matrices and the networks that use the lower rank weight matrices appears to disappear when the evaluation code is run for the second time in a row, which would be consistent with the above hypothesis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7jlMYxhi7E-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "c0480380-9a23-478e-8b60-a2b696de4b36"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "## TODO: add your code below to plot the relationship between time and rank percentage\n",
        "\n",
        "ranks = [x[0] for x in ranks_and_times] # Extract the rank values from the ranks_and_times list of tuples\n",
        "latencies = [x[1] for x in ranks_and_times] # Extract the latency values from the ranks_and_times list of tuples\n",
        "plt.plot(ranks,latencies) # Create a plot of ranks versus latencies\n",
        "plt.xlabel('rank fraction') # Label the axes\n",
        "plt.ylabel('latency') # Label the axes \n",
        "plt.show() # Display the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR3JTDWb2QII"
      },
      "source": [
        "## Coding Challenge Part 3: Perform evaluations on the dataset in factorized space. (4 points)\n",
        "\n",
        "In this section, you will perform evaluations on the dataset in factorized space.\n",
        "\n",
        "* [**4 points**] 2 pts for question 6 and question 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OctdUxerSZut"
      },
      "outputs": [],
      "source": [
        "\n",
        "def low_rank_net_fn(batch: Batch, rank: float) -> jnp.ndarray:\n",
        "  \n",
        "  x = normalize(batch[0])\n",
        "  total_input_dim = np.prod(x.shape[1:])\n",
        "\n",
        "  # Do not alter the architecture code.\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(int(rank * min(total_input_dim, 3000)), with_bias=False),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 2000), with_bias=False), \n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 2000), with_bias=False), \n",
        "      hk.Linear(2000), jax.nn.relu,      \n",
        "      hk.Linear(int(rank * 1000), with_bias=False), \n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(int(rank * 10), with_bias=False),\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi2TN_WV-jgZ",
        "outputId": "fed58a84-c1b6-43cf-d2d5-e50cfff3531b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model at 1.00\n",
            "Rank Fraction / Test accuracy: 1.00 / 0.626.\n",
            "Rank Fraction / Duration: 1.00 / 0.7493.\n",
            "Evaluating the model at 0.90\n",
            "Rank Fraction / Test accuracy: 0.90 / 0.625.\n",
            "Rank Fraction / Duration: 0.90 / 0.8440.\n",
            "Evaluating the model at 0.80\n",
            "Rank Fraction / Test accuracy: 0.80 / 0.626.\n",
            "Rank Fraction / Duration: 0.80 / 0.7439.\n",
            "Evaluating the model at 0.70\n",
            "Rank Fraction / Test accuracy: 0.70 / 0.625.\n",
            "Rank Fraction / Duration: 0.70 / 0.6799.\n",
            "Evaluating the model at 0.60\n",
            "Rank Fraction / Test accuracy: 0.60 / 0.598.\n",
            "Rank Fraction / Duration: 0.60 / 0.6392.\n",
            "Evaluating the model at 0.50\n",
            "Rank Fraction / Test accuracy: 0.50 / 0.557.\n",
            "Rank Fraction / Duration: 0.50 / 0.5410.\n",
            "Evaluating the model at 0.40\n",
            "Rank Fraction / Test accuracy: 0.40 / 0.507.\n",
            "Rank Fraction / Duration: 0.40 / 0.4734.\n",
            "Evaluating the model at 0.30\n",
            "Rank Fraction / Test accuracy: 0.30 / 0.435.\n",
            "Rank Fraction / Duration: 0.30 / 0.4360.\n",
            "Evaluating the model at 0.20\n",
            "Rank Fraction / Test accuracy: 0.20 / 0.357.\n",
            "Rank Fraction / Duration: 0.20 / 0.3885.\n",
            "Evaluating the model at 0.10\n",
            "Rank Fraction / Test accuracy: 0.10 / 0.191.\n",
            "Rank Fraction / Duration: 0.10 / 0.3865.\n"
          ]
        }
      ],
      "source": [
        "vanilla_to_low_rank_map = {\n",
        "    'conv2_d': 'conv2_d',\n",
        "    'conv2_d_1': 'conv2_d_1',\n",
        "    'linear': ['linear', 'linear_1'],\n",
        "    'linear_1': ['linear_2', 'linear_3'],\n",
        "    'linear_2': ['linear_4', 'linear_5'],\n",
        "    'linear_3': ['linear_6', 'linear_7'],\n",
        "    'linear_4': ['linear_8', 'linear_9']\n",
        "}\n",
        "\n",
        "\n",
        "ranks_and_accuracies = []\n",
        "ranks_and_times = []\n",
        "for rank_fraction in np.arange(1.0, 0.0, -0.1):\n",
        "  low_rank_net_fn_partial = partial(low_rank_net_fn, rank=rank_fraction)\n",
        "  net = hk.without_apply_rng(hk.transform(low_rank_net_fn_partial)) \n",
        "  low_rank_params = net.init(jax.random.PRNGKey(42), next(train))\n",
        "\n",
        "  print(f\"Evaluating the model at \" f\"{rank_fraction:.2f}\")\n",
        "\n",
        "  for layer in vanilla_to_low_rank_map.keys():\n",
        "    if 'conv' in layer:\n",
        "      low_rank_params[layer] = params[layer]\n",
        "      continue\n",
        "    weight = params[layer]['w']\n",
        "    # TODO: complete coding the rank_approximated_weight function to compute the SVD of the matrix to return the rank approximated weights u and v for a given matrix.\n",
        "    u, v = rank_approximated_weight(weight, rank_fraction)\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][0]]['w'] = u\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][1]]['w'] = v\n",
        "    low_rank_params[vanilla_to_low_rank_map[layer][1]]['b'] = params[layer]['b']\n",
        "  \n",
        "  # TODO: modify the compute_eval_metrics function below to compute the time taken for inference.\n",
        "  test_accuracy, duration = compute_eval_metrics(low_rank_params, next(test), 50)\n",
        "  ranks_and_times.append((rank_fraction, np.mean(duration)))\n",
        "  ranks_and_accuracies.append((rank_fraction, np.mean(test_accuracy)))\n",
        "  print(f\"Rank Fraction / Test accuracy: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(test_accuracy):.3f}.\")\n",
        "  print(f\"Rank Fraction / Duration: \"\n",
        "          f\"{rank_fraction:.2f} / {np.mean(duration):.4f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObBn-Pf_996r"
      },
      "source": [
        "### Q6: Plot a curve showing time vs rank percentage of the matrix "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0bJJFL4LM7q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "677dbcb3-94a9-47d0-930a-5c9399a58962"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# TODO: add code to plot the relationship between time vs percentage rank of the matrix.\n",
        "\n",
        "ranks = [x[0] for x in ranks_and_times] # Extract the rank values from the ranks_and_times list of tuples\n",
        "latencies = [x[1] for x in ranks_and_times] # Extract the latency values from the ranks_and_times list of tuples\n",
        "plt.plot(ranks,latencies) # Create a plot of ranks versus latencies\n",
        "plt.xlabel('rank') # Label the axes\n",
        "plt.ylabel('latency') # Label the axes \n",
        "plt.show() # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "313ALwDu93k1"
      },
      "source": [
        "### Q7: What do you observe between time and the percentage rank of the matrix.\n",
        "\n",
        "### Put your answer here: \n",
        "\n",
        "Latency tends to decrease as the rank fraction of the linear layer weight matrices decreases. This makes sense, since the code above uses the factorized matrices for inference, so the lower rank approximations will tend to have linear layer weight matrices with a smaller overall memory footprint than higher rank approximations (and therefore will tend to require less memory references for inference).\n",
        "\n",
        "Interestingly, it seems like latency tends to be higher for networks using factorized linear layer weight matrices for inference than for networks that use non-factorized weight matrices until the rank fraction gets quite low (e.g. less than 0.5). For rank fractions below 0.5, it seems like the latency for factorized networks tends to be less than for any of the non-factorized networks, but at the cost of significantly lower accuracy. Perhaps these patterns are due in part to the fact that for high rank fractions it is possible for the factorized matrices to actually have a collectively larger memory footprint than the equivalent non-factorized matrix (it may also take more arithmetic operations to multiply a vector by the factorized matrices than the equivalent non-factorized matrix for high rank fractions). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHcBKkogM6uV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEAu9Vu-0rWX"
      },
      "source": [
        "## Coding Challenge Part 4: Take this Further (10 bonus points)\n",
        "\n",
        "This part of the challenge is designed to be open ended. If you wanted to show some more skills, here is your chance to shine. We include two options below -- **only do one of the options**:\n",
        "\n",
        "**Option 1:** Implement a change that isn't SVD but minimizes inference latency while preserving accuracy. Can you outperform SVD? \n",
        "\n",
        "\n",
        "\n",
        "**Option 2:** Improve the quality of code for this takehome. Pretend you are reviewing a peer and add comments to cells with suggestions of how to improve the code quality. Try and make your comments action orientated and precise. \n",
        "\n",
        "\n",
        "**For Option 1, DO NOT alter the previous code sections, instead add any new code below. You should not need to add new code for Option 2, instead just add comments to cells.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE FROM RAVI: I added comments/suggestions related to improving the quality of the code for this takehome as stuff occurred to me as I was working on the problems above (frankly, I did this in part because I wasn't sure how long it would take me to figure out how to do Option 1 (and therefore wasn't sure whether I'd have time to do it)). However, I also did Option 1 below since I wanted to try out another compression approach. If you need to pick one of the options to \"grade,\" please pick my work for Option 1 below. Thank you!  "
      ],
      "metadata": {
        "id": "G69xrYX4LoMG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoN-IhY9hNQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7912e0d-5794-4e50-c416-3a3dbf78a4b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized accuracy: 0.625999927520752\n",
            "Quantized latency: 0.416282414300008\n",
            "Non-quantized accuracy: 0.6259000301361084\n",
            "Non-quantized latency: 0.5359369176800101\n"
          ]
        }
      ],
      "source": [
        "# TODO: add code for option 1 here\n",
        "\n",
        "\"\"\"\n",
        "The code in this cell quantizes the parameters of the trained neural network\n",
        "from fp32 to fp16 and compares the accuracy and latency of the model using \n",
        "the original non-quantized parameters with the model using the quantized \n",
        "parameters. \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "The function below defines the same convolutional neural network architecture\n",
        "used in Part 1 of the Coding Challenge, but quantizes the image input data \n",
        "from float32 to float16 so that it is compatible with the quantized parameters. \n",
        "\"\"\"\n",
        "def quant_net_fn(batch: Batch) -> jnp.ndarray:\n",
        "\n",
        "  x = normalize(batch[0]) # normalization and standardization of image data before it is input into neural network\n",
        "  x = x.astype(jnp.float16) # quantize input data from float32 to float16 \n",
        "\n",
        "  # Do NOT alter the architecture definition below.\n",
        "  net = hk.Sequential([\n",
        "      hk.Conv2D(output_channels=6*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      jax.nn.relu,\n",
        "      hk.Conv2D(output_channels=16*3, kernel_shape=(5,5)),\n",
        "      jax.nn.relu,\n",
        "      hk.AvgPool(window_shape=(2,2), strides=(2,2), padding='VALID'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(3000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(2000), jax.nn.relu,\n",
        "      hk.Linear(1000), jax.nn.relu,\n",
        "      hk.Linear(10),\n",
        "  ])\n",
        "  return net(x)\n",
        "\n",
        "\"\"\"\n",
        "The function below is a version of the compute_accuracy function above that uses\n",
        "quant_net to compute accuracy instead of net.\n",
        "\n",
        "RAVI'S COMMENT: It would probably be better if compute_accuracy was defined so that\n",
        "the model function is passed to it as an argument as opposed to referenced as a global\n",
        "variable. That way, the compute_accuracy function could be reused for different\n",
        "model architectures rather than needing to create a new compute_accuracy function\n",
        "for each model architecture you want to evaluate. \n",
        "\"\"\"\n",
        "def quant_compute_accuracy(params: hk.Params, batch: Batch) -> jnp.ndarray:\n",
        "  predictions = quant_net.apply(params, batch) # Compute logits for each example in batch\n",
        "\n",
        "  # TODO: add code below to compute the accuracy over the batch.\n",
        " \n",
        "  \"\"\"\n",
        "  The line below computes accuracy by: (1) calculating the index of the column with \n",
        "  the highest logit value for each prediction (which corresponds to the number of \n",
        "  the CIFAR10 category that the image is most likely to fall into, according to the\n",
        "  prediction), (2) comparing the index from (1) to the \"true\" category number, and\n",
        "  (3) calculating the fraction of predictions that are correct.\n",
        "  \"\"\"\n",
        "  accuracy = jnp.mean(jnp.argmax(predictions, axis=1) == batch[1])\n",
        "  return accuracy\n",
        "\n",
        "\"\"\"\n",
        "The function below is a version of the compute_eval_metrics function above that uses\n",
        "quant_net to compute the evaluation metrics instead of net.\n",
        "\n",
        "RAVI'S COMMENT: It would probably be better if compute_eval_metrics was defined so that\n",
        "the model function is passed to it as an argument as opposed to referenced as a global\n",
        "variable. That way, the compute_eval_metrics function could be reused for different\n",
        "model architectures rather than needing to create a new compute_eval_metrics function\n",
        "for each model architecture you want to evaluate. \n",
        "\"\"\"\n",
        "def quant_compute_eval_metrics(params, batch, n_samples):\n",
        "# TODO: add code to compute the time for inference.\n",
        "  duration_list = []\n",
        "  accuracy_list = []\n",
        "  for _ in range(n_samples):\n",
        "    start = time.perf_counter() # Get the time immediately before running inference\n",
        "    predictions = quant_net.apply(params, batch) # Compute logits for each example in batch\n",
        "    duration = time.perf_counter() - start # Get the time immediately after running inference and subtract from start time to get an estimate of inference latency\n",
        "    acc = quant_compute_accuracy(params, batch) # Compute accuracy for batch. \n",
        "    duration_list.append(duration) # Append most recent latency measurement to list of latency measurements\n",
        "    accuracy_list.append(acc) # Append most recent accuracy measurement to list of accuracy measurements\n",
        "\n",
        "  return accuracy_list, duration_list\n",
        "\n",
        "net = hk.without_apply_rng(hk.transform(net_fn))\n",
        "quant_net = hk.without_apply_rng(hk.transform(quant_net_fn)) \n",
        "quantized_params = quant_net.init(jax.random.PRNGKey(42), next(train)) # Initialize quantized model parameters\n",
        "quantized_params = jax.tree_util.tree_map(lambda x: x.astype(jnp.float16), params) # Create a pytree of quantized parameters\n",
        "quant_compute_eval_metrics(quantized_params, next(test), 10) # Run quant_compute_eval_metrics once so that code and data that is reused for for each inference is loaded before measuring latency \n",
        "compute_eval_metrics(params, next(test), 10) # Run compute_eval_metrics once so that code and data that is reused for for each inference is loaded before measuring latency \n",
        "orig_test_accuracy, orig_duration = compute_eval_metrics(params, next(test), 50) # Compute accuracy and latency for original non-quantized parameters\n",
        "quant_test_accuracy, quant_duration = quant_compute_eval_metrics(quantized_params, next(test), 50) # Compute accuracy and latency for quantized parameters\n",
        "\n",
        "print(f\"Quantized accuracy: {np.mean(quant_test_accuracy)}\")\n",
        "print(f\"Quantized latency: {np.mean(quant_duration)}\")\n",
        "print(f\"Non-quantized accuracy: {np.mean(orig_test_accuracy)}\")\n",
        "print(f\"Non-quantized latency: {np.mean(orig_duration)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAVI'S COMMENTS ON RESULTS OF ABOVE CODE: \n",
        "\n",
        "It seems like quantizing the model parameters from 32 bit floating point to 16 bit floating point enables a substantial reduction in latency (typically around 20%) with virtually no reduction in accuracy. This compares favorably to the low rank approximation approach, where substantial reductions in latency were only obtained at the cost of a large loss in accuracy. "
      ],
      "metadata": {
        "id": "VSPy-pF6NVTt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq2zcXoRNkYx"
      },
      "source": [
        "## You have made it to the end of the challenge!\n",
        "\n",
        "Before you submit your completed challenge document, please make sure to **save and pin your revisions** before submitting a link to your submission via the [Cohere For AI Scholars Program Application.](https://jobs.lever.co/cohere/) "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-v-iHTH3nasL",
        "UHGqpn3tkFRL"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a008cf05978447c488da8d273be40900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d956211357364f28b13304d2a389438c",
              "IPY_MODEL_6cfac5d568814b11bc6b9b05aa0f2e40",
              "IPY_MODEL_60bc3ed4c92948328ad28220d2b890aa"
            ],
            "layout": "IPY_MODEL_415f3f66e6c44a3f96b503309d136e8f"
          }
        },
        "d956211357364f28b13304d2a389438c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20178df3a17443d4a4d9716db32fd864",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_39b4cb5ba9594f309bcac087b8d9cf51",
            "value": "Dl Completed...: 100%"
          }
        },
        "6cfac5d568814b11bc6b9b05aa0f2e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ad0702206644bcfadab9b129fb8aa85",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_30165e701353434a97cb9acff4ec2c46",
            "value": 1
          }
        },
        "60bc3ed4c92948328ad28220d2b890aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9be3b4f925d44ccf97adb231362a6ae3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c87965aab61f4ae7aa9f88e56c507ba0",
            "value": " 1/1 [00:17&lt;00:00, 15.45s/ url]"
          }
        },
        "415f3f66e6c44a3f96b503309d136e8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20178df3a17443d4a4d9716db32fd864": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39b4cb5ba9594f309bcac087b8d9cf51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ad0702206644bcfadab9b129fb8aa85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "30165e701353434a97cb9acff4ec2c46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9be3b4f925d44ccf97adb231362a6ae3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c87965aab61f4ae7aa9f88e56c507ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1923b659376b4db1b143e702469c7496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d1b8f5c7e2c4e88973be10585249a68",
              "IPY_MODEL_9bbb7e12070041ffbe7f1cb5e71d551f",
              "IPY_MODEL_adf024f3ad3846baafffbfb5af72e396"
            ],
            "layout": "IPY_MODEL_68b69e94a7fd4a8caff8794c7a864ae8"
          }
        },
        "2d1b8f5c7e2c4e88973be10585249a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_275b17134b6e4ac0a77bb4234403dfb2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f4d697c6703b4c76b9262043bd715203",
            "value": "Dl Size...: 100%"
          }
        },
        "9bbb7e12070041ffbe7f1cb5e71d551f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4fb078a5e3c461bad5e4dba53291b3f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56cef49d8a1c494ba7f84832a6ddd264",
            "value": 1
          }
        },
        "adf024f3ad3846baafffbfb5af72e396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbc91c9b31794b2eb8cd722be6d03608",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c23bd2e2768c46ce8eebf13da2900fb4",
            "value": " 162/162 [00:17&lt;00:00, 12.70 MiB/s]"
          }
        },
        "68b69e94a7fd4a8caff8794c7a864ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "275b17134b6e4ac0a77bb4234403dfb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4d697c6703b4c76b9262043bd715203": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4fb078a5e3c461bad5e4dba53291b3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "56cef49d8a1c494ba7f84832a6ddd264": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbc91c9b31794b2eb8cd722be6d03608": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c23bd2e2768c46ce8eebf13da2900fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57a011ad7492448fab9978965ce775ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f7d3f1b8927940c3a589ec69a08716fe",
              "IPY_MODEL_7f2ed601d0e541aca8573babaa719900",
              "IPY_MODEL_134658d4cd324e5b92469d7c643427dc"
            ],
            "layout": "IPY_MODEL_e43d4f807dbe43fdb0c5667159b66cec"
          }
        },
        "f7d3f1b8927940c3a589ec69a08716fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aee34abfcfa94145ab86dbec3874f2d8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_50a77f27cb1f4f14aa978fabfdafa5f9",
            "value": "Extraction completed...: 100%"
          }
        },
        "7f2ed601d0e541aca8573babaa719900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009d67202aef44d9a4e623ad124d1e31",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78cbd879a42c4bf587b3185f55e7a504",
            "value": 1
          }
        },
        "134658d4cd324e5b92469d7c643427dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4e3ae0285b8498fb54ec8dfcb4f45b9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fbf5e8399e8c489da4298d55da89088f",
            "value": " 1/1 [00:17&lt;00:00, 17.13s/ file]"
          }
        },
        "e43d4f807dbe43fdb0c5667159b66cec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee34abfcfa94145ab86dbec3874f2d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50a77f27cb1f4f14aa978fabfdafa5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "009d67202aef44d9a4e623ad124d1e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "78cbd879a42c4bf587b3185f55e7a504": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4e3ae0285b8498fb54ec8dfcb4f45b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf5e8399e8c489da4298d55da89088f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97405f587fc6488e94872c927fc3655e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_465d5ba9ae8e400f8ed8c386961ef4d2",
              "IPY_MODEL_2be28ed8a87f4c1c9e26ac177f1958d4",
              "IPY_MODEL_9de69f8763ca44a0bca423693e1eb6fa"
            ],
            "layout": "IPY_MODEL_d0c8763260164b69b5ca49b21f2b9ee4"
          }
        },
        "465d5ba9ae8e400f8ed8c386961ef4d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bc62f7a876a4ca5bd60c5c8b545dbe8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_83bc1a5a2d0842bd9184f194e575ec26",
            "value": "Generating splits...: 100%"
          }
        },
        "2be28ed8a87f4c1c9e26ac177f1958d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d148e98c91f49f9986455ede7dcb70b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6a4128bab3f94fd39b86ce0b8b6393bf",
            "value": 2
          }
        },
        "9de69f8763ca44a0bca423693e1eb6fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7dff65e7cdc42b7a2a040d50949586f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c5eaa8de99954f6a96d51d233b13af7b",
            "value": " 2/2 [00:53&lt;00:00, 23.46s/ splits]"
          }
        },
        "d0c8763260164b69b5ca49b21f2b9ee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "4bc62f7a876a4ca5bd60c5c8b545dbe8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83bc1a5a2d0842bd9184f194e575ec26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d148e98c91f49f9986455ede7dcb70b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4128bab3f94fd39b86ce0b8b6393bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7dff65e7cdc42b7a2a040d50949586f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5eaa8de99954f6a96d51d233b13af7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5d429ebddba4bd8b0988d382e55fca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ca10413c6c384941888f6cc88da6b676",
              "IPY_MODEL_0246e59074cf4a53a2be5b3de81d38d4",
              "IPY_MODEL_a06489bfb9e04edf8f7521a0963f8c03"
            ],
            "layout": "IPY_MODEL_8fbd45dc85fb44f489e0828142aa00ea"
          }
        },
        "ca10413c6c384941888f6cc88da6b676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bbae03d22c048148db3d4f1aa7a771e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8192f6ba122442b9aa2eee9f73f6981a",
            "value": "Generating train examples...: 100%"
          }
        },
        "0246e59074cf4a53a2be5b3de81d38d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfe66aae46894809ba902e7a1f2da6aa",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5dacc67acdb426f8c0ea29ecad0343e",
            "value": 50000
          }
        },
        "a06489bfb9e04edf8f7521a0963f8c03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e9ebca1e0e4c2f91cbf479460a2ebb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_367a5729bb3146b7ae84604fcbd8254e",
            "value": " 49925/50000 [00:44&lt;00:00, 1202.80 examples/s]"
          }
        },
        "8fbd45dc85fb44f489e0828142aa00ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "9bbae03d22c048148db3d4f1aa7a771e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8192f6ba122442b9aa2eee9f73f6981a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfe66aae46894809ba902e7a1f2da6aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5dacc67acdb426f8c0ea29ecad0343e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e6e9ebca1e0e4c2f91cbf479460a2ebb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367a5729bb3146b7ae84604fcbd8254e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ff2faaaa36d4067944d7ff4fbdf8349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3288ff2bbedf455fbffe21434c113848",
              "IPY_MODEL_4193de256a3848f38088f83dcddc972d",
              "IPY_MODEL_eb754e8fd8de4a22aacc93c7d27b0d18"
            ],
            "layout": "IPY_MODEL_9378006e425e452381e3dd12c78e3d5d"
          }
        },
        "3288ff2bbedf455fbffe21434c113848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_972bef0331e344ecbfafc3f0746a5d3a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8422e3485b0a4f659ab7d2a5d3ae28f2",
            "value": "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteTDR4PH/cifar10-train.tfrecord*...:  89%"
          }
        },
        "4193de256a3848f38088f83dcddc972d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_957eae27b93d4e678c45605b7290d5d6",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b3a22f9cce54e3bbaa3fd91bc16f46f",
            "value": 50000
          }
        },
        "eb754e8fd8de4a22aacc93c7d27b0d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_364f1fbc51dd4a0397a9ee051e9f7aa6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4603e2cb0cd84ccd81b513c5d21795ec",
            "value": " 44349/50000 [00:00&lt;00:00, 165537.13 examples/s]"
          }
        },
        "9378006e425e452381e3dd12c78e3d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "972bef0331e344ecbfafc3f0746a5d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8422e3485b0a4f659ab7d2a5d3ae28f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "957eae27b93d4e678c45605b7290d5d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b3a22f9cce54e3bbaa3fd91bc16f46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "364f1fbc51dd4a0397a9ee051e9f7aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4603e2cb0cd84ccd81b513c5d21795ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8a19fae63a842ca8d96efee0dc57b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ceb056b8f034355ac6a1fd06a0427f8",
              "IPY_MODEL_379c566312d342cdbe88dd05786a9de0",
              "IPY_MODEL_032597a75e484d88877019227b4f0a45"
            ],
            "layout": "IPY_MODEL_109506268cbb4d189e5f3211cbee219a"
          }
        },
        "9ceb056b8f034355ac6a1fd06a0427f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9402317f9f3147d7847891559e303474",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b63eab95ce0b416d98619b6d2fc35ed7",
            "value": "Generating test examples...:  99%"
          }
        },
        "379c566312d342cdbe88dd05786a9de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d42e6ed4514d4b579b01d3b1a351ce3f",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78c82749f85a4503a5b0b1e9361b7be4",
            "value": 10000
          }
        },
        "032597a75e484d88877019227b4f0a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08ee4e788ca4fa2acaa771022b7bc5a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d8b0c53da29a40a0a32a651a845c6ff6",
            "value": " 9886/10000 [00:08&lt;00:00, 1186.48 examples/s]"
          }
        },
        "109506268cbb4d189e5f3211cbee219a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "9402317f9f3147d7847891559e303474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b63eab95ce0b416d98619b6d2fc35ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d42e6ed4514d4b579b01d3b1a351ce3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c82749f85a4503a5b0b1e9361b7be4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f08ee4e788ca4fa2acaa771022b7bc5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b0c53da29a40a0a32a651a845c6ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b004acfe68f548d2a3122af09fa6fea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a7421529bc294d10a9c7a40aa04bcab9",
              "IPY_MODEL_f7b82acda00c4b7481fc449bc8b55ba0",
              "IPY_MODEL_943a80a684ea49578bfa9f823f0a2701"
            ],
            "layout": "IPY_MODEL_f60575b314874b47bbe046362b1fdda5"
          }
        },
        "a7421529bc294d10a9c7a40aa04bcab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d760140394a74285aeac403b6f6d9241",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_63bd73d14003497ca828a3af175b22ec",
            "value": "Shuffling ~/tensorflow_datasets/cifar10/3.0.2.incompleteTDR4PH/cifar10-test.tfrecord*...:  97%"
          }
        },
        "f7b82acda00c4b7481fc449bc8b55ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45e0c193d8054ec688957807ce41b41e",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da89e20930a54612a5926b49d80526b1",
            "value": 10000
          }
        },
        "943a80a684ea49578bfa9f823f0a2701": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcbe4de60a49493e988076c05e3cc0fb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2339a1dce506453981ed84f44d317b60",
            "value": " 9684/10000 [00:00&lt;00:00, 96827.86 examples/s]"
          }
        },
        "f60575b314874b47bbe046362b1fdda5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d760140394a74285aeac403b6f6d9241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63bd73d14003497ca828a3af175b22ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45e0c193d8054ec688957807ce41b41e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da89e20930a54612a5926b49d80526b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcbe4de60a49493e988076c05e3cc0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2339a1dce506453981ed84f44d317b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}